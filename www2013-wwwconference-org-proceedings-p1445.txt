 a typical way is to assume that a short document only covers a single topic. by learning btm, we can obtain the topic components and a global topic distribution of the corpus, except the topic distribution of each individual document as it does not model the document generation process. in plsa, a document is presented as a mixture of topics, while a topic is a probability distribution over words. however, mixture of unigrams assumes that all the words in a document are sampled from the same topic. in this way, btm not only can keep the correlation between words, but also can capture multiple topic gradients in a document, since the topic assignments of di erent biterms in a document are independent. therefore, we cannot directly obtain the topic proportions of documents during the topic learning process. 38.07s btm 128.64s 74.38s 250.07s 108.13s 362.27s 143.47s 476.19 s 178.66s 591.24s both btm for each topic, besides the top 20 words, which are  here we study the quality of topics discovered by the two topic models. in this paper, we propose a novel probabilistic topic model for short texts, namely biterm topic model (btm).